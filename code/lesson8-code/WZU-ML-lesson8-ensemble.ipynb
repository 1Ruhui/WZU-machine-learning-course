{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8f4f85",
   "metadata": {},
   "source": [
    "# 机器学习练习8 集成学习\n",
    "\n",
    "代码修改并注释：黄海广，haiguang2000@wzu.edu.cn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3257f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc304344",
   "metadata": {},
   "source": [
    "## 生成数据\n",
    "生成12000行的数据，训练集和测试集按照3:1划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebd8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "data, target = make_hastie_10_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6dfeb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 10), (3000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ac5c7",
   "metadata": {},
   "source": [
    "## 模型对比\n",
    "对比六大模型，都使用默认参数，因为数据是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f808ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51200000 (+/- 0.02),耗时0.04秒。模型名称[Logistic Regression]\n",
      "Accuracy: 0.88222222 (+/- 0.01),耗时15.51秒。模型名称[Random Forest]\n",
      "Accuracy: 0.87377778 (+/- 0.01),耗时3.11秒。模型名称[AdaBoost]\n",
      "Accuracy: 0.91444444 (+/- 0.01),耗时12.64秒。模型名称[GBDT]\n",
      "[22:08:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:08:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:08:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:08:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:08:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.92400000 (+/- 0.00),耗时3.55秒。模型名称[XGBoost]\n",
      "Accuracy: 0.93200000 (+/- 0.01),耗时0.67秒。模型名称[LightGBM]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf6 = LGBMClassifier()\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], [\n",
    "        'Logistic Regression', 'Random Forest', 'AdaBoost', 'GBDT', 'XGBoost',\n",
    "        'LightGBM'\n",
    "]):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    end = time.time()\n",
    "    running_time = end - start\n",
    "    print(\"Accuracy: %0.8f (+/- %0.2f),耗时%0.2f秒。模型名称[%s]\" %\n",
    "          (scores.mean(), scores.std(), running_time, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa60220",
   "metadata": {},
   "source": [
    "对比了六大模型，可以看出，逻辑回归速度最快，但准确率最低。\n",
    "而LightGBM，速度快，而且准确率最高，所以，现在处理结构化数据的时候，大部分都是用LightGBM算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e7313",
   "metadata": {},
   "source": [
    "## XGBoost的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5b733",
   "metadata": {},
   "source": [
    "### 1.原生XGBoost的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d20084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#记录程序运行时间\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#xgb矩阵赋值\n",
    "xgb_train = xgb.DMatrix(X_train, y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "##参数\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'silent': 1,  #设置成1则没有运行信息输出，最好是设置为0.\n",
    "    #'nthread':7,# cpu 线程数 默认最大\n",
    "    'eta': 0.007,  # 如同学习率\n",
    "    'min_child_weight': 3,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    'max_depth': 6,  # 构建树的深度，越大越容易过拟合\n",
    "    'gamma': 0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    'subsample': 0.7,  # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,  # 生成树时进行的列采样 \n",
    "    'lambda': 2,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #'alpha':0, # L1 正则项参数\n",
    "    #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。\n",
    "    #'objective': 'multi:softmax', #多分类的问题\n",
    "    #'num_class':10, # 类别数，多分类与 multisoftmax 并用\n",
    "    'seed': 1000,  #随机种子\n",
    "    #'eval_metric': 'auc'\n",
    "}\n",
    "plst = list(params.items())\n",
    "num_rounds = 500  # 迭代次数\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7421bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:08:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1.11867\tval-rmse:1.10739\n",
      "[1]\ttrain-rmse:1.11594\tval-rmse:1.10488\n",
      "[2]\ttrain-rmse:1.11322\tval-rmse:1.10231\n",
      "[3]\ttrain-rmse:1.11046\tval-rmse:1.09976\n",
      "[4]\ttrain-rmse:1.10784\tval-rmse:1.09732\n",
      "[5]\ttrain-rmse:1.10518\tval-rmse:1.09492\n",
      "[6]\ttrain-rmse:1.10251\tval-rmse:1.09247\n",
      "[7]\ttrain-rmse:1.09984\tval-rmse:1.08999\n",
      "[8]\ttrain-rmse:1.09725\tval-rmse:1.08757\n",
      "[9]\ttrain-rmse:1.09460\tval-rmse:1.08517\n",
      "[10]\ttrain-rmse:1.09209\tval-rmse:1.08286\n",
      "[11]\ttrain-rmse:1.08948\tval-rmse:1.08055\n",
      "[12]\ttrain-rmse:1.08692\tval-rmse:1.07824\n",
      "[13]\ttrain-rmse:1.08438\tval-rmse:1.07597\n",
      "[14]\ttrain-rmse:1.08182\tval-rmse:1.07362\n",
      "[15]\ttrain-rmse:1.07940\tval-rmse:1.07142\n",
      "[16]\ttrain-rmse:1.07695\tval-rmse:1.06917\n",
      "[17]\ttrain-rmse:1.07458\tval-rmse:1.06693\n",
      "[18]\ttrain-rmse:1.07211\tval-rmse:1.06469\n",
      "[19]\ttrain-rmse:1.06960\tval-rmse:1.06242\n",
      "[20]\ttrain-rmse:1.06723\tval-rmse:1.06021\n",
      "[21]\ttrain-rmse:1.06494\tval-rmse:1.05813\n",
      "[22]\ttrain-rmse:1.06250\tval-rmse:1.05599\n",
      "[23]\ttrain-rmse:1.06015\tval-rmse:1.05386\n",
      "[24]\ttrain-rmse:1.05788\tval-rmse:1.05179\n",
      "[25]\ttrain-rmse:1.05559\tval-rmse:1.04963\n",
      "[26]\ttrain-rmse:1.05324\tval-rmse:1.04759\n",
      "[27]\ttrain-rmse:1.05100\tval-rmse:1.04552\n",
      "[28]\ttrain-rmse:1.04872\tval-rmse:1.04353\n",
      "[29]\ttrain-rmse:1.04646\tval-rmse:1.04153\n",
      "[30]\ttrain-rmse:1.04425\tval-rmse:1.03955\n",
      "[31]\ttrain-rmse:1.04194\tval-rmse:1.03744\n",
      "[32]\ttrain-rmse:1.03968\tval-rmse:1.03537\n",
      "[33]\ttrain-rmse:1.03755\tval-rmse:1.03345\n",
      "[34]\ttrain-rmse:1.03540\tval-rmse:1.03152\n",
      "[35]\ttrain-rmse:1.03323\tval-rmse:1.02950\n",
      "[36]\ttrain-rmse:1.03107\tval-rmse:1.02754\n",
      "[37]\ttrain-rmse:1.02896\tval-rmse:1.02563\n",
      "[38]\ttrain-rmse:1.02683\tval-rmse:1.02372\n",
      "[39]\ttrain-rmse:1.02473\tval-rmse:1.02187\n",
      "[40]\ttrain-rmse:1.02266\tval-rmse:1.02008\n",
      "[41]\ttrain-rmse:1.02059\tval-rmse:1.01822\n",
      "[42]\ttrain-rmse:1.01854\tval-rmse:1.01635\n",
      "[43]\ttrain-rmse:1.01651\tval-rmse:1.01448\n",
      "[44]\ttrain-rmse:1.01449\tval-rmse:1.01267\n",
      "[45]\ttrain-rmse:1.01249\tval-rmse:1.01090\n",
      "[46]\ttrain-rmse:1.01050\tval-rmse:1.00908\n",
      "[47]\ttrain-rmse:1.00845\tval-rmse:1.00721\n",
      "[48]\ttrain-rmse:1.00653\tval-rmse:1.00556\n",
      "[49]\ttrain-rmse:1.00454\tval-rmse:1.00375\n",
      "[50]\ttrain-rmse:1.00256\tval-rmse:1.00198\n",
      "[51]\ttrain-rmse:1.00068\tval-rmse:1.00032\n",
      "[52]\ttrain-rmse:0.99873\tval-rmse:0.99861\n",
      "[53]\ttrain-rmse:0.99674\tval-rmse:0.99690\n",
      "[54]\ttrain-rmse:0.99493\tval-rmse:0.99524\n",
      "[55]\ttrain-rmse:0.99307\tval-rmse:0.99357\n",
      "[56]\ttrain-rmse:0.99121\tval-rmse:0.99190\n",
      "[57]\ttrain-rmse:0.98931\tval-rmse:0.99017\n",
      "[58]\ttrain-rmse:0.98741\tval-rmse:0.98841\n",
      "[59]\ttrain-rmse:0.98561\tval-rmse:0.98681\n",
      "[60]\ttrain-rmse:0.98376\tval-rmse:0.98516\n",
      "[61]\ttrain-rmse:0.98190\tval-rmse:0.98350\n",
      "[62]\ttrain-rmse:0.98009\tval-rmse:0.98188\n",
      "[63]\ttrain-rmse:0.97835\tval-rmse:0.98036\n",
      "[64]\ttrain-rmse:0.97665\tval-rmse:0.97879\n",
      "[65]\ttrain-rmse:0.97490\tval-rmse:0.97722\n",
      "[66]\ttrain-rmse:0.97313\tval-rmse:0.97561\n",
      "[67]\ttrain-rmse:0.97143\tval-rmse:0.97408\n",
      "[68]\ttrain-rmse:0.96971\tval-rmse:0.97252\n",
      "[69]\ttrain-rmse:0.96797\tval-rmse:0.97097\n",
      "[70]\ttrain-rmse:0.96621\tval-rmse:0.96941\n",
      "[71]\ttrain-rmse:0.96455\tval-rmse:0.96793\n",
      "[72]\ttrain-rmse:0.96278\tval-rmse:0.96642\n",
      "[73]\ttrain-rmse:0.96106\tval-rmse:0.96486\n",
      "[74]\ttrain-rmse:0.95940\tval-rmse:0.96344\n",
      "[75]\ttrain-rmse:0.95777\tval-rmse:0.96202\n",
      "[76]\ttrain-rmse:0.95616\tval-rmse:0.96058\n",
      "[77]\ttrain-rmse:0.95447\tval-rmse:0.95905\n",
      "[78]\ttrain-rmse:0.95281\tval-rmse:0.95759\n",
      "[79]\ttrain-rmse:0.95113\tval-rmse:0.95611\n",
      "[80]\ttrain-rmse:0.94955\tval-rmse:0.95469\n",
      "[81]\ttrain-rmse:0.94793\tval-rmse:0.95331\n",
      "[82]\ttrain-rmse:0.94628\tval-rmse:0.95185\n",
      "[83]\ttrain-rmse:0.94475\tval-rmse:0.95057\n",
      "[84]\ttrain-rmse:0.94318\tval-rmse:0.94916\n",
      "[85]\ttrain-rmse:0.94166\tval-rmse:0.94779\n",
      "[86]\ttrain-rmse:0.94013\tval-rmse:0.94641\n",
      "[87]\ttrain-rmse:0.93861\tval-rmse:0.94512\n",
      "[88]\ttrain-rmse:0.93705\tval-rmse:0.94381\n",
      "[89]\ttrain-rmse:0.93554\tval-rmse:0.94252\n",
      "[90]\ttrain-rmse:0.93392\tval-rmse:0.94110\n",
      "[91]\ttrain-rmse:0.93243\tval-rmse:0.93984\n",
      "[92]\ttrain-rmse:0.93093\tval-rmse:0.93850\n",
      "[93]\ttrain-rmse:0.92943\tval-rmse:0.93722\n",
      "[94]\ttrain-rmse:0.92794\tval-rmse:0.93594\n",
      "[95]\ttrain-rmse:0.92645\tval-rmse:0.93460\n",
      "[96]\ttrain-rmse:0.92499\tval-rmse:0.93330\n",
      "[97]\ttrain-rmse:0.92350\tval-rmse:0.93202\n",
      "[98]\ttrain-rmse:0.92204\tval-rmse:0.93074\n",
      "[99]\ttrain-rmse:0.92059\tval-rmse:0.92940\n",
      "[100]\ttrain-rmse:0.91920\tval-rmse:0.92825\n",
      "[101]\ttrain-rmse:0.91780\tval-rmse:0.92701\n",
      "[102]\ttrain-rmse:0.91641\tval-rmse:0.92583\n",
      "[103]\ttrain-rmse:0.91498\tval-rmse:0.92452\n",
      "[104]\ttrain-rmse:0.91353\tval-rmse:0.92323\n",
      "[105]\ttrain-rmse:0.91209\tval-rmse:0.92199\n",
      "[106]\ttrain-rmse:0.91072\tval-rmse:0.92076\n",
      "[107]\ttrain-rmse:0.90938\tval-rmse:0.91965\n",
      "[108]\ttrain-rmse:0.90798\tval-rmse:0.91848\n",
      "[109]\ttrain-rmse:0.90665\tval-rmse:0.91730\n",
      "[110]\ttrain-rmse:0.90529\tval-rmse:0.91606\n",
      "[111]\ttrain-rmse:0.90394\tval-rmse:0.91492\n",
      "[112]\ttrain-rmse:0.90266\tval-rmse:0.91377\n",
      "[113]\ttrain-rmse:0.90130\tval-rmse:0.91264\n",
      "[114]\ttrain-rmse:0.89994\tval-rmse:0.91152\n",
      "[115]\ttrain-rmse:0.89862\tval-rmse:0.91037\n",
      "[116]\ttrain-rmse:0.89726\tval-rmse:0.90923\n",
      "[117]\ttrain-rmse:0.89600\tval-rmse:0.90814\n",
      "[118]\ttrain-rmse:0.89471\tval-rmse:0.90700\n",
      "[119]\ttrain-rmse:0.89341\tval-rmse:0.90588\n",
      "[120]\ttrain-rmse:0.89206\tval-rmse:0.90471\n",
      "[121]\ttrain-rmse:0.89071\tval-rmse:0.90348\n",
      "[122]\ttrain-rmse:0.88937\tval-rmse:0.90227\n",
      "[123]\ttrain-rmse:0.88808\tval-rmse:0.90112\n",
      "[124]\ttrain-rmse:0.88682\tval-rmse:0.90005\n",
      "[125]\ttrain-rmse:0.88550\tval-rmse:0.89885\n",
      "[126]\ttrain-rmse:0.88422\tval-rmse:0.89775\n",
      "[127]\ttrain-rmse:0.88294\tval-rmse:0.89668\n",
      "[128]\ttrain-rmse:0.88172\tval-rmse:0.89563\n",
      "[129]\ttrain-rmse:0.88044\tval-rmse:0.89451\n",
      "[130]\ttrain-rmse:0.87922\tval-rmse:0.89348\n",
      "[131]\ttrain-rmse:0.87797\tval-rmse:0.89236\n",
      "[132]\ttrain-rmse:0.87669\tval-rmse:0.89128\n",
      "[133]\ttrain-rmse:0.87542\tval-rmse:0.89018\n",
      "[134]\ttrain-rmse:0.87418\tval-rmse:0.88915\n",
      "[135]\ttrain-rmse:0.87300\tval-rmse:0.88815\n",
      "[136]\ttrain-rmse:0.87185\tval-rmse:0.88712\n",
      "[137]\ttrain-rmse:0.87068\tval-rmse:0.88614\n",
      "[138]\ttrain-rmse:0.86953\tval-rmse:0.88514\n",
      "[139]\ttrain-rmse:0.86832\tval-rmse:0.88406\n",
      "[140]\ttrain-rmse:0.86716\tval-rmse:0.88301\n",
      "[141]\ttrain-rmse:0.86603\tval-rmse:0.88198\n",
      "[142]\ttrain-rmse:0.86487\tval-rmse:0.88100\n",
      "[143]\ttrain-rmse:0.86372\tval-rmse:0.88002\n",
      "[144]\ttrain-rmse:0.86256\tval-rmse:0.87898\n",
      "[145]\ttrain-rmse:0.86141\tval-rmse:0.87805\n",
      "[146]\ttrain-rmse:0.86029\tval-rmse:0.87709\n",
      "[147]\ttrain-rmse:0.85917\tval-rmse:0.87609\n",
      "[148]\ttrain-rmse:0.85805\tval-rmse:0.87510\n",
      "[149]\ttrain-rmse:0.85691\tval-rmse:0.87414\n",
      "[150]\ttrain-rmse:0.85579\tval-rmse:0.87317\n",
      "[151]\ttrain-rmse:0.85469\tval-rmse:0.87223\n",
      "[152]\ttrain-rmse:0.85355\tval-rmse:0.87121\n",
      "[153]\ttrain-rmse:0.85244\tval-rmse:0.87030\n",
      "[154]\ttrain-rmse:0.85133\tval-rmse:0.86943\n",
      "[155]\ttrain-rmse:0.85020\tval-rmse:0.86847\n",
      "[156]\ttrain-rmse:0.84911\tval-rmse:0.86753\n",
      "[157]\ttrain-rmse:0.84800\tval-rmse:0.86663\n",
      "[158]\ttrain-rmse:0.84690\tval-rmse:0.86577\n",
      "[159]\ttrain-rmse:0.84582\tval-rmse:0.86483\n",
      "[160]\ttrain-rmse:0.84474\tval-rmse:0.86385\n",
      "[161]\ttrain-rmse:0.84364\tval-rmse:0.86298\n",
      "[162]\ttrain-rmse:0.84260\tval-rmse:0.86212\n",
      "[163]\ttrain-rmse:0.84155\tval-rmse:0.86119\n",
      "[164]\ttrain-rmse:0.84051\tval-rmse:0.86036\n",
      "[165]\ttrain-rmse:0.83946\tval-rmse:0.85954\n",
      "[166]\ttrain-rmse:0.83844\tval-rmse:0.85869\n",
      "[167]\ttrain-rmse:0.83740\tval-rmse:0.85783\n",
      "[168]\ttrain-rmse:0.83637\tval-rmse:0.85699\n",
      "[169]\ttrain-rmse:0.83537\tval-rmse:0.85619\n",
      "[170]\ttrain-rmse:0.83437\tval-rmse:0.85536\n",
      "[171]\ttrain-rmse:0.83336\tval-rmse:0.85449\n",
      "[172]\ttrain-rmse:0.83234\tval-rmse:0.85363\n",
      "[173]\ttrain-rmse:0.83130\tval-rmse:0.85270\n",
      "[174]\ttrain-rmse:0.83028\tval-rmse:0.85188\n",
      "[175]\ttrain-rmse:0.82923\tval-rmse:0.85103\n",
      "[176]\ttrain-rmse:0.82814\tval-rmse:0.85020\n",
      "[177]\ttrain-rmse:0.82713\tval-rmse:0.84936\n",
      "[178]\ttrain-rmse:0.82613\tval-rmse:0.84855\n",
      "[179]\ttrain-rmse:0.82510\tval-rmse:0.84769\n",
      "[180]\ttrain-rmse:0.82409\tval-rmse:0.84681\n",
      "[181]\ttrain-rmse:0.82311\tval-rmse:0.84601\n",
      "[182]\ttrain-rmse:0.82217\tval-rmse:0.84522\n",
      "[183]\ttrain-rmse:0.82120\tval-rmse:0.84442\n",
      "[184]\ttrain-rmse:0.82024\tval-rmse:0.84363\n",
      "[185]\ttrain-rmse:0.81923\tval-rmse:0.84282\n",
      "[186]\ttrain-rmse:0.81822\tval-rmse:0.84206\n",
      "[187]\ttrain-rmse:0.81718\tval-rmse:0.84126\n",
      "[188]\ttrain-rmse:0.81624\tval-rmse:0.84051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189]\ttrain-rmse:0.81530\tval-rmse:0.83966\n",
      "[190]\ttrain-rmse:0.81438\tval-rmse:0.83891\n",
      "[191]\ttrain-rmse:0.81347\tval-rmse:0.83818\n",
      "[192]\ttrain-rmse:0.81256\tval-rmse:0.83742\n",
      "[193]\ttrain-rmse:0.81163\tval-rmse:0.83663\n",
      "[194]\ttrain-rmse:0.81068\tval-rmse:0.83581\n",
      "[195]\ttrain-rmse:0.80973\tval-rmse:0.83504\n",
      "[196]\ttrain-rmse:0.80876\tval-rmse:0.83424\n",
      "[197]\ttrain-rmse:0.80781\tval-rmse:0.83346\n",
      "[198]\ttrain-rmse:0.80689\tval-rmse:0.83273\n",
      "[199]\ttrain-rmse:0.80593\tval-rmse:0.83189\n",
      "[200]\ttrain-rmse:0.80504\tval-rmse:0.83112\n",
      "[201]\ttrain-rmse:0.80417\tval-rmse:0.83036\n",
      "[202]\ttrain-rmse:0.80327\tval-rmse:0.82960\n",
      "[203]\ttrain-rmse:0.80236\tval-rmse:0.82886\n",
      "[204]\ttrain-rmse:0.80145\tval-rmse:0.82817\n",
      "[205]\ttrain-rmse:0.80051\tval-rmse:0.82739\n",
      "[206]\ttrain-rmse:0.79960\tval-rmse:0.82660\n",
      "[207]\ttrain-rmse:0.79875\tval-rmse:0.82595\n",
      "[208]\ttrain-rmse:0.79785\tval-rmse:0.82522\n",
      "[209]\ttrain-rmse:0.79696\tval-rmse:0.82451\n",
      "[210]\ttrain-rmse:0.79606\tval-rmse:0.82382\n",
      "[211]\ttrain-rmse:0.79513\tval-rmse:0.82310\n",
      "[212]\ttrain-rmse:0.79426\tval-rmse:0.82247\n",
      "[213]\ttrain-rmse:0.79337\tval-rmse:0.82170\n",
      "[214]\ttrain-rmse:0.79253\tval-rmse:0.82098\n",
      "[215]\ttrain-rmse:0.79166\tval-rmse:0.82026\n",
      "[216]\ttrain-rmse:0.79079\tval-rmse:0.81948\n",
      "[217]\ttrain-rmse:0.78995\tval-rmse:0.81883\n",
      "[218]\ttrain-rmse:0.78907\tval-rmse:0.81813\n",
      "[219]\ttrain-rmse:0.78826\tval-rmse:0.81751\n",
      "[220]\ttrain-rmse:0.78744\tval-rmse:0.81690\n",
      "[221]\ttrain-rmse:0.78656\tval-rmse:0.81618\n",
      "[222]\ttrain-rmse:0.78568\tval-rmse:0.81550\n",
      "[223]\ttrain-rmse:0.78483\tval-rmse:0.81483\n",
      "[224]\ttrain-rmse:0.78404\tval-rmse:0.81418\n",
      "[225]\ttrain-rmse:0.78323\tval-rmse:0.81350\n",
      "[226]\ttrain-rmse:0.78242\tval-rmse:0.81279\n",
      "[227]\ttrain-rmse:0.78155\tval-rmse:0.81205\n",
      "[228]\ttrain-rmse:0.78078\tval-rmse:0.81146\n",
      "[229]\ttrain-rmse:0.78000\tval-rmse:0.81080\n",
      "[230]\ttrain-rmse:0.77916\tval-rmse:0.81011\n",
      "[231]\ttrain-rmse:0.77836\tval-rmse:0.80953\n",
      "[232]\ttrain-rmse:0.77761\tval-rmse:0.80888\n",
      "[233]\ttrain-rmse:0.77682\tval-rmse:0.80822\n",
      "[234]\ttrain-rmse:0.77601\tval-rmse:0.80760\n",
      "[235]\ttrain-rmse:0.77519\tval-rmse:0.80692\n",
      "[236]\ttrain-rmse:0.77437\tval-rmse:0.80627\n",
      "[237]\ttrain-rmse:0.77355\tval-rmse:0.80553\n",
      "[238]\ttrain-rmse:0.77280\tval-rmse:0.80488\n",
      "[239]\ttrain-rmse:0.77199\tval-rmse:0.80420\n",
      "[240]\ttrain-rmse:0.77114\tval-rmse:0.80355\n",
      "[241]\ttrain-rmse:0.77033\tval-rmse:0.80290\n",
      "[242]\ttrain-rmse:0.76953\tval-rmse:0.80222\n",
      "[243]\ttrain-rmse:0.76879\tval-rmse:0.80159\n",
      "[244]\ttrain-rmse:0.76802\tval-rmse:0.80094\n",
      "[245]\ttrain-rmse:0.76724\tval-rmse:0.80031\n",
      "[246]\ttrain-rmse:0.76644\tval-rmse:0.79975\n",
      "[247]\ttrain-rmse:0.76562\tval-rmse:0.79908\n",
      "[248]\ttrain-rmse:0.76484\tval-rmse:0.79846\n",
      "[249]\ttrain-rmse:0.76406\tval-rmse:0.79778\n",
      "[250]\ttrain-rmse:0.76327\tval-rmse:0.79714\n",
      "[251]\ttrain-rmse:0.76248\tval-rmse:0.79657\n",
      "[252]\ttrain-rmse:0.76172\tval-rmse:0.79594\n",
      "[253]\ttrain-rmse:0.76095\tval-rmse:0.79536\n",
      "[254]\ttrain-rmse:0.76015\tval-rmse:0.79475\n",
      "[255]\ttrain-rmse:0.75939\tval-rmse:0.79414\n",
      "[256]\ttrain-rmse:0.75869\tval-rmse:0.79351\n",
      "[257]\ttrain-rmse:0.75794\tval-rmse:0.79291\n",
      "[258]\ttrain-rmse:0.75717\tval-rmse:0.79231\n",
      "[259]\ttrain-rmse:0.75649\tval-rmse:0.79179\n",
      "[260]\ttrain-rmse:0.75572\tval-rmse:0.79115\n",
      "[261]\ttrain-rmse:0.75497\tval-rmse:0.79053\n",
      "[262]\ttrain-rmse:0.75424\tval-rmse:0.78994\n",
      "[263]\ttrain-rmse:0.75351\tval-rmse:0.78935\n",
      "[264]\ttrain-rmse:0.75279\tval-rmse:0.78878\n",
      "[265]\ttrain-rmse:0.75201\tval-rmse:0.78816\n",
      "[266]\ttrain-rmse:0.75129\tval-rmse:0.78758\n",
      "[267]\ttrain-rmse:0.75058\tval-rmse:0.78698\n",
      "[268]\ttrain-rmse:0.74982\tval-rmse:0.78633\n",
      "[269]\ttrain-rmse:0.74912\tval-rmse:0.78575\n",
      "[270]\ttrain-rmse:0.74841\tval-rmse:0.78517\n",
      "[271]\ttrain-rmse:0.74763\tval-rmse:0.78453\n",
      "[272]\ttrain-rmse:0.74691\tval-rmse:0.78401\n",
      "[273]\ttrain-rmse:0.74616\tval-rmse:0.78340\n",
      "[274]\ttrain-rmse:0.74544\tval-rmse:0.78287\n",
      "[275]\ttrain-rmse:0.74476\tval-rmse:0.78229\n",
      "[276]\ttrain-rmse:0.74405\tval-rmse:0.78175\n",
      "[277]\ttrain-rmse:0.74335\tval-rmse:0.78121\n",
      "[278]\ttrain-rmse:0.74265\tval-rmse:0.78067\n",
      "[279]\ttrain-rmse:0.74193\tval-rmse:0.78011\n",
      "[280]\ttrain-rmse:0.74113\tval-rmse:0.77945\n",
      "[281]\ttrain-rmse:0.74047\tval-rmse:0.77892\n",
      "[282]\ttrain-rmse:0.73976\tval-rmse:0.77836\n",
      "[283]\ttrain-rmse:0.73908\tval-rmse:0.77778\n",
      "[284]\ttrain-rmse:0.73841\tval-rmse:0.77723\n",
      "[285]\ttrain-rmse:0.73774\tval-rmse:0.77668\n",
      "[286]\ttrain-rmse:0.73709\tval-rmse:0.77613\n",
      "[287]\ttrain-rmse:0.73645\tval-rmse:0.77564\n",
      "[288]\ttrain-rmse:0.73575\tval-rmse:0.77519\n",
      "[289]\ttrain-rmse:0.73506\tval-rmse:0.77465\n",
      "[290]\ttrain-rmse:0.73433\tval-rmse:0.77404\n",
      "[291]\ttrain-rmse:0.73362\tval-rmse:0.77351\n",
      "[292]\ttrain-rmse:0.73294\tval-rmse:0.77298\n",
      "[293]\ttrain-rmse:0.73228\tval-rmse:0.77246\n",
      "[294]\ttrain-rmse:0.73159\tval-rmse:0.77194\n",
      "[295]\ttrain-rmse:0.73090\tval-rmse:0.77143\n",
      "[296]\ttrain-rmse:0.73026\tval-rmse:0.77093\n",
      "[297]\ttrain-rmse:0.72957\tval-rmse:0.77045\n",
      "[298]\ttrain-rmse:0.72889\tval-rmse:0.76990\n",
      "[299]\ttrain-rmse:0.72818\tval-rmse:0.76938\n",
      "[300]\ttrain-rmse:0.72756\tval-rmse:0.76885\n",
      "[301]\ttrain-rmse:0.72694\tval-rmse:0.76833\n",
      "[302]\ttrain-rmse:0.72628\tval-rmse:0.76782\n",
      "[303]\ttrain-rmse:0.72565\tval-rmse:0.76735\n",
      "[304]\ttrain-rmse:0.72496\tval-rmse:0.76683\n",
      "[305]\ttrain-rmse:0.72428\tval-rmse:0.76627\n",
      "[306]\ttrain-rmse:0.72360\tval-rmse:0.76573\n",
      "[307]\ttrain-rmse:0.72291\tval-rmse:0.76525\n",
      "[308]\ttrain-rmse:0.72231\tval-rmse:0.76474\n",
      "[309]\ttrain-rmse:0.72165\tval-rmse:0.76421\n",
      "[310]\ttrain-rmse:0.72101\tval-rmse:0.76373\n",
      "[311]\ttrain-rmse:0.72040\tval-rmse:0.76323\n",
      "[312]\ttrain-rmse:0.71978\tval-rmse:0.76273\n",
      "[313]\ttrain-rmse:0.71917\tval-rmse:0.76219\n",
      "[314]\ttrain-rmse:0.71851\tval-rmse:0.76170\n",
      "[315]\ttrain-rmse:0.71784\tval-rmse:0.76119\n",
      "[316]\ttrain-rmse:0.71725\tval-rmse:0.76070\n",
      "[317]\ttrain-rmse:0.71658\tval-rmse:0.76020\n",
      "[318]\ttrain-rmse:0.71595\tval-rmse:0.75973\n",
      "[319]\ttrain-rmse:0.71531\tval-rmse:0.75924\n",
      "[320]\ttrain-rmse:0.71467\tval-rmse:0.75873\n",
      "[321]\ttrain-rmse:0.71399\tval-rmse:0.75824\n",
      "[322]\ttrain-rmse:0.71334\tval-rmse:0.75770\n",
      "[323]\ttrain-rmse:0.71270\tval-rmse:0.75721\n",
      "[324]\ttrain-rmse:0.71215\tval-rmse:0.75673\n",
      "[325]\ttrain-rmse:0.71151\tval-rmse:0.75626\n",
      "[326]\ttrain-rmse:0.71089\tval-rmse:0.75578\n",
      "[327]\ttrain-rmse:0.71022\tval-rmse:0.75530\n",
      "[328]\ttrain-rmse:0.70957\tval-rmse:0.75479\n",
      "[329]\ttrain-rmse:0.70894\tval-rmse:0.75424\n",
      "[330]\ttrain-rmse:0.70831\tval-rmse:0.75377\n",
      "[331]\ttrain-rmse:0.70773\tval-rmse:0.75329\n",
      "[332]\ttrain-rmse:0.70716\tval-rmse:0.75287\n",
      "[333]\ttrain-rmse:0.70652\tval-rmse:0.75238\n",
      "[334]\ttrain-rmse:0.70590\tval-rmse:0.75189\n",
      "[335]\ttrain-rmse:0.70532\tval-rmse:0.75141\n",
      "[336]\ttrain-rmse:0.70473\tval-rmse:0.75098\n",
      "[337]\ttrain-rmse:0.70409\tval-rmse:0.75042\n",
      "[338]\ttrain-rmse:0.70348\tval-rmse:0.74992\n",
      "[339]\ttrain-rmse:0.70291\tval-rmse:0.74952\n",
      "[340]\ttrain-rmse:0.70231\tval-rmse:0.74908\n",
      "[341]\ttrain-rmse:0.70174\tval-rmse:0.74860\n",
      "[342]\ttrain-rmse:0.70119\tval-rmse:0.74818\n",
      "[343]\ttrain-rmse:0.70059\tval-rmse:0.74767\n",
      "[344]\ttrain-rmse:0.70001\tval-rmse:0.74719\n",
      "[345]\ttrain-rmse:0.69941\tval-rmse:0.74677\n",
      "[346]\ttrain-rmse:0.69885\tval-rmse:0.74631\n",
      "[347]\ttrain-rmse:0.69826\tval-rmse:0.74589\n",
      "[348]\ttrain-rmse:0.69770\tval-rmse:0.74547\n",
      "[349]\ttrain-rmse:0.69711\tval-rmse:0.74508\n",
      "[350]\ttrain-rmse:0.69653\tval-rmse:0.74464\n",
      "[351]\ttrain-rmse:0.69594\tval-rmse:0.74418\n",
      "[352]\ttrain-rmse:0.69539\tval-rmse:0.74377\n",
      "[353]\ttrain-rmse:0.69480\tval-rmse:0.74334\n",
      "[354]\ttrain-rmse:0.69425\tval-rmse:0.74283\n",
      "[355]\ttrain-rmse:0.69364\tval-rmse:0.74234\n",
      "[356]\ttrain-rmse:0.69309\tval-rmse:0.74192\n",
      "[357]\ttrain-rmse:0.69248\tval-rmse:0.74141\n",
      "[358]\ttrain-rmse:0.69192\tval-rmse:0.74099\n",
      "[359]\ttrain-rmse:0.69134\tval-rmse:0.74052\n",
      "[360]\ttrain-rmse:0.69079\tval-rmse:0.74013\n",
      "[361]\ttrain-rmse:0.69023\tval-rmse:0.73967\n",
      "[362]\ttrain-rmse:0.68968\tval-rmse:0.73926\n",
      "[363]\ttrain-rmse:0.68917\tval-rmse:0.73886\n",
      "[364]\ttrain-rmse:0.68860\tval-rmse:0.73845\n",
      "[365]\ttrain-rmse:0.68806\tval-rmse:0.73800\n",
      "[366]\ttrain-rmse:0.68751\tval-rmse:0.73756\n",
      "[367]\ttrain-rmse:0.68699\tval-rmse:0.73719\n",
      "[368]\ttrain-rmse:0.68646\tval-rmse:0.73670\n",
      "[369]\ttrain-rmse:0.68591\tval-rmse:0.73628\n",
      "[370]\ttrain-rmse:0.68538\tval-rmse:0.73585\n",
      "[371]\ttrain-rmse:0.68485\tval-rmse:0.73545\n",
      "[372]\ttrain-rmse:0.68428\tval-rmse:0.73499\n",
      "[373]\ttrain-rmse:0.68371\tval-rmse:0.73462\n",
      "[374]\ttrain-rmse:0.68313\tval-rmse:0.73420\n",
      "[375]\ttrain-rmse:0.68262\tval-rmse:0.73378\n",
      "[376]\ttrain-rmse:0.68206\tval-rmse:0.73339\n",
      "[377]\ttrain-rmse:0.68153\tval-rmse:0.73300\n",
      "[378]\ttrain-rmse:0.68103\tval-rmse:0.73257\n",
      "[379]\ttrain-rmse:0.68051\tval-rmse:0.73222\n",
      "[380]\ttrain-rmse:0.68000\tval-rmse:0.73183\n",
      "[381]\ttrain-rmse:0.67944\tval-rmse:0.73140\n",
      "[382]\ttrain-rmse:0.67891\tval-rmse:0.73101\n",
      "[383]\ttrain-rmse:0.67841\tval-rmse:0.73063\n",
      "[384]\ttrain-rmse:0.67787\tval-rmse:0.73025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[385]\ttrain-rmse:0.67733\tval-rmse:0.72984\n",
      "[386]\ttrain-rmse:0.67679\tval-rmse:0.72948\n",
      "[387]\ttrain-rmse:0.67628\tval-rmse:0.72911\n",
      "[388]\ttrain-rmse:0.67576\tval-rmse:0.72872\n",
      "[389]\ttrain-rmse:0.67524\tval-rmse:0.72824\n",
      "[390]\ttrain-rmse:0.67468\tval-rmse:0.72786\n",
      "[391]\ttrain-rmse:0.67415\tval-rmse:0.72746\n",
      "[392]\ttrain-rmse:0.67362\tval-rmse:0.72708\n",
      "[393]\ttrain-rmse:0.67309\tval-rmse:0.72671\n",
      "[394]\ttrain-rmse:0.67255\tval-rmse:0.72631\n",
      "[395]\ttrain-rmse:0.67201\tval-rmse:0.72590\n",
      "[396]\ttrain-rmse:0.67147\tval-rmse:0.72547\n",
      "[397]\ttrain-rmse:0.67093\tval-rmse:0.72502\n",
      "[398]\ttrain-rmse:0.67038\tval-rmse:0.72460\n",
      "[399]\ttrain-rmse:0.66988\tval-rmse:0.72420\n",
      "[400]\ttrain-rmse:0.66935\tval-rmse:0.72383\n",
      "[401]\ttrain-rmse:0.66883\tval-rmse:0.72341\n",
      "[402]\ttrain-rmse:0.66831\tval-rmse:0.72299\n",
      "[403]\ttrain-rmse:0.66777\tval-rmse:0.72262\n",
      "[404]\ttrain-rmse:0.66725\tval-rmse:0.72225\n",
      "[405]\ttrain-rmse:0.66676\tval-rmse:0.72184\n",
      "[406]\ttrain-rmse:0.66622\tval-rmse:0.72150\n",
      "[407]\ttrain-rmse:0.66571\tval-rmse:0.72111\n",
      "[408]\ttrain-rmse:0.66521\tval-rmse:0.72072\n",
      "[409]\ttrain-rmse:0.66473\tval-rmse:0.72038\n",
      "[410]\ttrain-rmse:0.66422\tval-rmse:0.72004\n",
      "[411]\ttrain-rmse:0.66370\tval-rmse:0.71969\n",
      "[412]\ttrain-rmse:0.66318\tval-rmse:0.71935\n",
      "[413]\ttrain-rmse:0.66268\tval-rmse:0.71894\n",
      "[414]\ttrain-rmse:0.66220\tval-rmse:0.71857\n",
      "[415]\ttrain-rmse:0.66172\tval-rmse:0.71824\n",
      "[416]\ttrain-rmse:0.66123\tval-rmse:0.71791\n",
      "[417]\ttrain-rmse:0.66071\tval-rmse:0.71754\n",
      "[418]\ttrain-rmse:0.66021\tval-rmse:0.71720\n",
      "[419]\ttrain-rmse:0.65970\tval-rmse:0.71681\n",
      "[420]\ttrain-rmse:0.65919\tval-rmse:0.71646\n",
      "[421]\ttrain-rmse:0.65870\tval-rmse:0.71611\n",
      "[422]\ttrain-rmse:0.65820\tval-rmse:0.71575\n",
      "[423]\ttrain-rmse:0.65774\tval-rmse:0.71540\n",
      "[424]\ttrain-rmse:0.65726\tval-rmse:0.71505\n",
      "[425]\ttrain-rmse:0.65680\tval-rmse:0.71468\n",
      "[426]\ttrain-rmse:0.65635\tval-rmse:0.71433\n",
      "[427]\ttrain-rmse:0.65589\tval-rmse:0.71398\n",
      "[428]\ttrain-rmse:0.65539\tval-rmse:0.71362\n",
      "[429]\ttrain-rmse:0.65490\tval-rmse:0.71323\n",
      "[430]\ttrain-rmse:0.65443\tval-rmse:0.71291\n",
      "[431]\ttrain-rmse:0.65395\tval-rmse:0.71262\n",
      "[432]\ttrain-rmse:0.65339\tval-rmse:0.71227\n",
      "[433]\ttrain-rmse:0.65291\tval-rmse:0.71194\n",
      "[434]\ttrain-rmse:0.65245\tval-rmse:0.71156\n",
      "[435]\ttrain-rmse:0.65198\tval-rmse:0.71121\n",
      "[436]\ttrain-rmse:0.65147\tval-rmse:0.71084\n",
      "[437]\ttrain-rmse:0.65102\tval-rmse:0.71050\n",
      "[438]\ttrain-rmse:0.65055\tval-rmse:0.71017\n",
      "[439]\ttrain-rmse:0.65009\tval-rmse:0.70985\n",
      "[440]\ttrain-rmse:0.64965\tval-rmse:0.70954\n",
      "[441]\ttrain-rmse:0.64920\tval-rmse:0.70919\n",
      "[442]\ttrain-rmse:0.64873\tval-rmse:0.70886\n",
      "[443]\ttrain-rmse:0.64821\tval-rmse:0.70848\n",
      "[444]\ttrain-rmse:0.64780\tval-rmse:0.70818\n",
      "[445]\ttrain-rmse:0.64734\tval-rmse:0.70786\n",
      "[446]\ttrain-rmse:0.64687\tval-rmse:0.70753\n",
      "[447]\ttrain-rmse:0.64639\tval-rmse:0.70717\n",
      "[448]\ttrain-rmse:0.64597\tval-rmse:0.70693\n",
      "[449]\ttrain-rmse:0.64549\tval-rmse:0.70657\n",
      "[450]\ttrain-rmse:0.64504\tval-rmse:0.70623\n",
      "[451]\ttrain-rmse:0.64460\tval-rmse:0.70589\n",
      "[452]\ttrain-rmse:0.64416\tval-rmse:0.70561\n",
      "[453]\ttrain-rmse:0.64373\tval-rmse:0.70526\n",
      "[454]\ttrain-rmse:0.64330\tval-rmse:0.70496\n",
      "[455]\ttrain-rmse:0.64280\tval-rmse:0.70459\n",
      "[456]\ttrain-rmse:0.64238\tval-rmse:0.70429\n",
      "[457]\ttrain-rmse:0.64192\tval-rmse:0.70397\n",
      "[458]\ttrain-rmse:0.64145\tval-rmse:0.70360\n",
      "[459]\ttrain-rmse:0.64104\tval-rmse:0.70327\n",
      "[460]\ttrain-rmse:0.64061\tval-rmse:0.70296\n",
      "[461]\ttrain-rmse:0.64018\tval-rmse:0.70269\n",
      "[462]\ttrain-rmse:0.63975\tval-rmse:0.70236\n",
      "[463]\ttrain-rmse:0.63929\tval-rmse:0.70206\n",
      "[464]\ttrain-rmse:0.63886\tval-rmse:0.70169\n",
      "[465]\ttrain-rmse:0.63843\tval-rmse:0.70136\n",
      "[466]\ttrain-rmse:0.63794\tval-rmse:0.70102\n",
      "[467]\ttrain-rmse:0.63752\tval-rmse:0.70071\n",
      "[468]\ttrain-rmse:0.63706\tval-rmse:0.70040\n",
      "[469]\ttrain-rmse:0.63663\tval-rmse:0.70008\n",
      "[470]\ttrain-rmse:0.63623\tval-rmse:0.69978\n",
      "[471]\ttrain-rmse:0.63576\tval-rmse:0.69950\n",
      "[472]\ttrain-rmse:0.63529\tval-rmse:0.69917\n",
      "[473]\ttrain-rmse:0.63485\tval-rmse:0.69889\n",
      "[474]\ttrain-rmse:0.63446\tval-rmse:0.69858\n",
      "[475]\ttrain-rmse:0.63398\tval-rmse:0.69826\n",
      "[476]\ttrain-rmse:0.63359\tval-rmse:0.69797\n",
      "[477]\ttrain-rmse:0.63318\tval-rmse:0.69769\n",
      "[478]\ttrain-rmse:0.63279\tval-rmse:0.69745\n",
      "[479]\ttrain-rmse:0.63232\tval-rmse:0.69715\n",
      "[480]\ttrain-rmse:0.63193\tval-rmse:0.69689\n",
      "[481]\ttrain-rmse:0.63147\tval-rmse:0.69654\n",
      "[482]\ttrain-rmse:0.63105\tval-rmse:0.69620\n",
      "[483]\ttrain-rmse:0.63062\tval-rmse:0.69591\n",
      "[484]\ttrain-rmse:0.63013\tval-rmse:0.69558\n",
      "[485]\ttrain-rmse:0.62974\tval-rmse:0.69528\n",
      "[486]\ttrain-rmse:0.62931\tval-rmse:0.69498\n",
      "[487]\ttrain-rmse:0.62891\tval-rmse:0.69469\n",
      "[488]\ttrain-rmse:0.62848\tval-rmse:0.69437\n",
      "[489]\ttrain-rmse:0.62806\tval-rmse:0.69400\n",
      "[490]\ttrain-rmse:0.62760\tval-rmse:0.69364\n",
      "[491]\ttrain-rmse:0.62722\tval-rmse:0.69336\n",
      "[492]\ttrain-rmse:0.62677\tval-rmse:0.69308\n",
      "[493]\ttrain-rmse:0.62633\tval-rmse:0.69274\n",
      "[494]\ttrain-rmse:0.62593\tval-rmse:0.69249\n",
      "[495]\ttrain-rmse:0.62555\tval-rmse:0.69222\n",
      "[496]\ttrain-rmse:0.62508\tval-rmse:0.69187\n",
      "[497]\ttrain-rmse:0.62465\tval-rmse:0.69156\n",
      "[498]\ttrain-rmse:0.62421\tval-rmse:0.69126\n",
      "[499]\ttrain-rmse:0.62383\tval-rmse:0.69098\n",
      "best best_ntree_limit 500\n",
      "error=0.842333\n",
      "xgboost success! \n",
      " cost time: 4.062307119369507 (s)......\n"
     ]
    }
   ],
   "source": [
    "#训练模型并保存\n",
    "# early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练\n",
    "model = xgb.train(\n",
    "    plst,\n",
    "    xgb_train,\n",
    "    num_rounds,\n",
    "    watchlist,\n",
    "    early_stopping_rounds=100,\n",
    ")\n",
    "#model.save_model('./model/xgb.model') # 用于存储训练出的模型\n",
    "print(\"best best_ntree_limit\", model.best_ntree_limit)\n",
    "y_pred = model.predict(xgb_test, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "# print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, y_pred))\n",
    "print('error=%f' %\n",
    "      (sum(1\n",
    "           for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) /\n",
    "       float(len(y_pred))))\n",
    "#输出运行时长\n",
    "cost_time = time.time() - start_time\n",
    "print(\"xgboost success!\", '\\n', \"cost time:\", cost_time, \"(s)......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee89f1",
   "metadata": {},
   "source": [
    "### 2.使用scikit-learn接口\n",
    "会改变的函数名是：\n",
    "\n",
    "eta -> learning_rate\n",
    "\n",
    "lambda -> reg_lambda\n",
    "\n",
    "alpha -> reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee9b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:08:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy : 0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "#     silent=0,  #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "    #nthread=4,# cpu 线程数 默认最大\n",
    "    learning_rate=0.3,  # 如同学习率\n",
    "    min_child_weight=1,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    max_depth=6,  # 构建树的深度，越大越容易过拟合\n",
    "    gamma=0,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    subsample=1,  # 随机采样训练样本 训练实例的子采样比\n",
    "    max_delta_step=0,  #最大增量步长，我们允许每个树的权重估计。\n",
    "    colsample_bytree=1,  # 生成树时进行的列采样 \n",
    "    reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #reg_alpha=0, # L1 正则项参数\n",
    "    #scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "    #objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "    #num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "    n_estimators=100,  #树的个数\n",
    "    seed=1000  #随机种子\n",
    "    #eval_metric= 'auc'\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "#设置验证集合 verbose=False不打印过程\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6196fe2",
   "metadata": {},
   "source": [
    "## LIghtGBM的使用\n",
    "### 1.原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fa87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -0.007778\n",
      "[1]\tvalid_0's l2: 0.963407\tvalid_0's auc: 0.822306\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's l2: 0.929732\tvalid_0's auc: 0.864549\n",
      "[3]\tvalid_0's l2: 0.899021\tvalid_0's auc: 0.878464\n",
      "[4]\tvalid_0's l2: 0.87243\tvalid_0's auc: 0.882184\n",
      "[5]\tvalid_0's l2: 0.84644\tvalid_0's auc: 0.892078\n",
      "[6]\tvalid_0's l2: 0.821495\tvalid_0's auc: 0.898242\n",
      "[7]\tvalid_0's l2: 0.799114\tvalid_0's auc: 0.902837\n",
      "[8]\tvalid_0's l2: 0.777408\tvalid_0's auc: 0.905669\n",
      "[9]\tvalid_0's l2: 0.758056\tvalid_0's auc: 0.910419\n",
      "[10]\tvalid_0's l2: 0.738781\tvalid_0's auc: 0.915105\n",
      "[11]\tvalid_0's l2: 0.720738\tvalid_0's auc: 0.917576\n",
      "[12]\tvalid_0's l2: 0.703566\tvalid_0's auc: 0.920586\n",
      "[13]\tvalid_0's l2: 0.690569\tvalid_0's auc: 0.921341\n",
      "[14]\tvalid_0's l2: 0.676317\tvalid_0's auc: 0.922679\n",
      "[15]\tvalid_0's l2: 0.663262\tvalid_0's auc: 0.925396\n",
      "[16]\tvalid_0's l2: 0.650168\tvalid_0's auc: 0.925978\n",
      "[17]\tvalid_0's l2: 0.63683\tvalid_0's auc: 0.9274\n",
      "[18]\tvalid_0's l2: 0.625148\tvalid_0's auc: 0.928321\n",
      "[19]\tvalid_0's l2: 0.614589\tvalid_0's auc: 0.929106\n",
      "[20]\tvalid_0's l2: 0.603415\tvalid_0's auc: 0.931287\n",
      "[21]\tvalid_0's l2: 0.592405\tvalid_0's auc: 0.932261\n",
      "[22]\tvalid_0's l2: 0.582218\tvalid_0's auc: 0.934936\n",
      "[23]\tvalid_0's l2: 0.573494\tvalid_0's auc: 0.935831\n",
      "[24]\tvalid_0's l2: 0.56501\tvalid_0's auc: 0.937179\n",
      "[25]\tvalid_0's l2: 0.556231\tvalid_0's auc: 0.939288\n",
      "[26]\tvalid_0's l2: 0.5482\tvalid_0's auc: 0.941986\n",
      "[27]\tvalid_0's l2: 0.540021\tvalid_0's auc: 0.943995\n",
      "[28]\tvalid_0's l2: 0.532454\tvalid_0's auc: 0.945517\n",
      "[29]\tvalid_0's l2: 0.525913\tvalid_0's auc: 0.946829\n",
      "[30]\tvalid_0's l2: 0.519463\tvalid_0's auc: 0.948082\n",
      "[31]\tvalid_0's l2: 0.5138\tvalid_0's auc: 0.949068\n",
      "[32]\tvalid_0's l2: 0.507896\tvalid_0's auc: 0.950236\n",
      "[33]\tvalid_0's l2: 0.501739\tvalid_0's auc: 0.950943\n",
      "[34]\tvalid_0's l2: 0.495844\tvalid_0's auc: 0.951919\n",
      "[35]\tvalid_0's l2: 0.490334\tvalid_0's auc: 0.952456\n",
      "[36]\tvalid_0's l2: 0.484837\tvalid_0's auc: 0.953192\n",
      "[37]\tvalid_0's l2: 0.47961\tvalid_0's auc: 0.954141\n",
      "[38]\tvalid_0's l2: 0.475399\tvalid_0's auc: 0.954832\n",
      "[39]\tvalid_0's l2: 0.470742\tvalid_0's auc: 0.955962\n",
      "[40]\tvalid_0's l2: 0.465878\tvalid_0's auc: 0.956823\n",
      "[41]\tvalid_0's l2: 0.461617\tvalid_0's auc: 0.957393\n",
      "[42]\tvalid_0's l2: 0.457525\tvalid_0's auc: 0.95819\n",
      "[43]\tvalid_0's l2: 0.45339\tvalid_0's auc: 0.959203\n",
      "[44]\tvalid_0's l2: 0.449194\tvalid_0's auc: 0.960162\n",
      "[45]\tvalid_0's l2: 0.446322\tvalid_0's auc: 0.960671\n",
      "[46]\tvalid_0's l2: 0.442829\tvalid_0's auc: 0.961551\n",
      "[47]\tvalid_0's l2: 0.438979\tvalid_0's auc: 0.962406\n",
      "[48]\tvalid_0's l2: 0.43605\tvalid_0's auc: 0.96296\n",
      "[49]\tvalid_0's l2: 0.432574\tvalid_0's auc: 0.963514\n",
      "[50]\tvalid_0's l2: 0.429017\tvalid_0's auc: 0.964151\n",
      "[51]\tvalid_0's l2: 0.425596\tvalid_0's auc: 0.964748\n",
      "[52]\tvalid_0's l2: 0.42196\tvalid_0's auc: 0.965369\n",
      "[53]\tvalid_0's l2: 0.418994\tvalid_0's auc: 0.966213\n",
      "[54]\tvalid_0's l2: 0.416561\tvalid_0's auc: 0.966547\n",
      "[55]\tvalid_0's l2: 0.413648\tvalid_0's auc: 0.967299\n",
      "[56]\tvalid_0's l2: 0.410593\tvalid_0's auc: 0.967722\n",
      "[57]\tvalid_0's l2: 0.408036\tvalid_0's auc: 0.9682\n",
      "[58]\tvalid_0's l2: 0.405561\tvalid_0's auc: 0.968551\n",
      "[59]\tvalid_0's l2: 0.40347\tvalid_0's auc: 0.969102\n",
      "[60]\tvalid_0's l2: 0.400647\tvalid_0's auc: 0.969412\n",
      "[61]\tvalid_0's l2: 0.397382\tvalid_0's auc: 0.969528\n",
      "[62]\tvalid_0's l2: 0.395402\tvalid_0's auc: 0.969815\n",
      "[63]\tvalid_0's l2: 0.392972\tvalid_0's auc: 0.970066\n",
      "[64]\tvalid_0's l2: 0.390606\tvalid_0's auc: 0.970435\n",
      "[65]\tvalid_0's l2: 0.388283\tvalid_0's auc: 0.970598\n",
      "[66]\tvalid_0's l2: 0.386125\tvalid_0's auc: 0.970937\n",
      "[67]\tvalid_0's l2: 0.383993\tvalid_0's auc: 0.971129\n",
      "[68]\tvalid_0's l2: 0.382279\tvalid_0's auc: 0.971397\n",
      "[69]\tvalid_0's l2: 0.38072\tvalid_0's auc: 0.971631\n",
      "[70]\tvalid_0's l2: 0.37892\tvalid_0's auc: 0.972002\n",
      "[71]\tvalid_0's l2: 0.376834\tvalid_0's auc: 0.972299\n",
      "[72]\tvalid_0's l2: 0.374846\tvalid_0's auc: 0.972681\n",
      "[73]\tvalid_0's l2: 0.373057\tvalid_0's auc: 0.972978\n",
      "[74]\tvalid_0's l2: 0.371298\tvalid_0's auc: 0.973396\n",
      "[75]\tvalid_0's l2: 0.369749\tvalid_0's auc: 0.973839\n",
      "[76]\tvalid_0's l2: 0.368154\tvalid_0's auc: 0.973989\n",
      "[77]\tvalid_0's l2: 0.366891\tvalid_0's auc: 0.974133\n",
      "[78]\tvalid_0's l2: 0.365695\tvalid_0's auc: 0.974317\n",
      "[79]\tvalid_0's l2: 0.364748\tvalid_0's auc: 0.974481\n",
      "[80]\tvalid_0's l2: 0.363737\tvalid_0's auc: 0.974661\n",
      "[81]\tvalid_0's l2: 0.362572\tvalid_0's auc: 0.974981\n",
      "[82]\tvalid_0's l2: 0.361431\tvalid_0's auc: 0.975228\n",
      "[83]\tvalid_0's l2: 0.360403\tvalid_0's auc: 0.975348\n",
      "[84]\tvalid_0's l2: 0.359323\tvalid_0's auc: 0.975533\n",
      "[85]\tvalid_0's l2: 0.358073\tvalid_0's auc: 0.975892\n",
      "[86]\tvalid_0's l2: 0.356658\tvalid_0's auc: 0.976201\n",
      "[87]\tvalid_0's l2: 0.355734\tvalid_0's auc: 0.976343\n",
      "[88]\tvalid_0's l2: 0.354593\tvalid_0's auc: 0.976593\n",
      "[89]\tvalid_0's l2: 0.353385\tvalid_0's auc: 0.976774\n",
      "[90]\tvalid_0's l2: 0.352786\tvalid_0's auc: 0.976891\n",
      "[91]\tvalid_0's l2: 0.351824\tvalid_0's auc: 0.977157\n",
      "[92]\tvalid_0's l2: 0.350948\tvalid_0's auc: 0.977392\n",
      "[93]\tvalid_0's l2: 0.350094\tvalid_0's auc: 0.977555\n",
      "[94]\tvalid_0's l2: 0.349159\tvalid_0's auc: 0.977807\n",
      "[95]\tvalid_0's l2: 0.34845\tvalid_0's auc: 0.977857\n",
      "[96]\tvalid_0's l2: 0.347614\tvalid_0's auc: 0.978066\n",
      "[97]\tvalid_0's l2: 0.346785\tvalid_0's auc: 0.978196\n",
      "[98]\tvalid_0's l2: 0.346117\tvalid_0's auc: 0.978319\n",
      "[99]\tvalid_0's l2: 0.345707\tvalid_0's auc: 0.978381\n",
      "[100]\tvalid_0's l2: 0.34523\tvalid_0's auc: 0.978401\n",
      "[101]\tvalid_0's l2: 0.344511\tvalid_0's auc: 0.978461\n",
      "[102]\tvalid_0's l2: 0.343855\tvalid_0's auc: 0.978582\n",
      "[103]\tvalid_0's l2: 0.343384\tvalid_0's auc: 0.97863\n",
      "[104]\tvalid_0's l2: 0.342492\tvalid_0's auc: 0.978858\n",
      "[105]\tvalid_0's l2: 0.341697\tvalid_0's auc: 0.979004\n",
      "[106]\tvalid_0's l2: 0.340912\tvalid_0's auc: 0.979181\n",
      "[107]\tvalid_0's l2: 0.340201\tvalid_0's auc: 0.979308\n",
      "[108]\tvalid_0's l2: 0.339426\tvalid_0's auc: 0.979485\n",
      "[109]\tvalid_0's l2: 0.33891\tvalid_0's auc: 0.979588\n",
      "[110]\tvalid_0's l2: 0.338107\tvalid_0's auc: 0.979724\n",
      "[111]\tvalid_0's l2: 0.337565\tvalid_0's auc: 0.979831\n",
      "[112]\tvalid_0's l2: 0.337141\tvalid_0's auc: 0.979866\n",
      "[113]\tvalid_0's l2: 0.33679\tvalid_0's auc: 0.979926\n",
      "[114]\tvalid_0's l2: 0.336336\tvalid_0's auc: 0.980022\n",
      "[115]\tvalid_0's l2: 0.335957\tvalid_0's auc: 0.980103\n",
      "[116]\tvalid_0's l2: 0.335552\tvalid_0's auc: 0.980281\n",
      "[117]\tvalid_0's l2: 0.335107\tvalid_0's auc: 0.980407\n",
      "[118]\tvalid_0's l2: 0.334645\tvalid_0's auc: 0.980555\n",
      "[119]\tvalid_0's l2: 0.33448\tvalid_0's auc: 0.980623\n",
      "[120]\tvalid_0's l2: 0.334393\tvalid_0's auc: 0.980664\n",
      "[121]\tvalid_0's l2: 0.334005\tvalid_0's auc: 0.980746\n",
      "[122]\tvalid_0's l2: 0.33339\tvalid_0's auc: 0.980918\n",
      "[123]\tvalid_0's l2: 0.333147\tvalid_0's auc: 0.980989\n",
      "[124]\tvalid_0's l2: 0.332794\tvalid_0's auc: 0.981065\n",
      "[125]\tvalid_0's l2: 0.332538\tvalid_0's auc: 0.981129\n",
      "[126]\tvalid_0's l2: 0.332218\tvalid_0's auc: 0.9812\n",
      "[127]\tvalid_0's l2: 0.331833\tvalid_0's auc: 0.981288\n",
      "[128]\tvalid_0's l2: 0.331691\tvalid_0's auc: 0.981309\n",
      "[129]\tvalid_0's l2: 0.331211\tvalid_0's auc: 0.981395\n",
      "[130]\tvalid_0's l2: 0.330984\tvalid_0's auc: 0.981422\n",
      "[131]\tvalid_0's l2: 0.33077\tvalid_0's auc: 0.981495\n",
      "[132]\tvalid_0's l2: 0.330556\tvalid_0's auc: 0.981518\n",
      "[133]\tvalid_0's l2: 0.330186\tvalid_0's auc: 0.981597\n",
      "[134]\tvalid_0's l2: 0.329996\tvalid_0's auc: 0.981647\n",
      "[135]\tvalid_0's l2: 0.32981\tvalid_0's auc: 0.981727\n",
      "[136]\tvalid_0's l2: 0.329645\tvalid_0's auc: 0.98174\n",
      "[137]\tvalid_0's l2: 0.329386\tvalid_0's auc: 0.981771\n",
      "[138]\tvalid_0's l2: 0.329113\tvalid_0's auc: 0.981824\n",
      "[139]\tvalid_0's l2: 0.32872\tvalid_0's auc: 0.981861\n",
      "[140]\tvalid_0's l2: 0.328479\tvalid_0's auc: 0.98196\n",
      "[141]\tvalid_0's l2: 0.328248\tvalid_0's auc: 0.981958\n",
      "[142]\tvalid_0's l2: 0.327952\tvalid_0's auc: 0.981983\n",
      "[143]\tvalid_0's l2: 0.327711\tvalid_0's auc: 0.982082\n",
      "[144]\tvalid_0's l2: 0.327458\tvalid_0's auc: 0.982091\n",
      "[145]\tvalid_0's l2: 0.327247\tvalid_0's auc: 0.982189\n",
      "[146]\tvalid_0's l2: 0.327277\tvalid_0's auc: 0.98222\n",
      "[147]\tvalid_0's l2: 0.327272\tvalid_0's auc: 0.982212\n",
      "[148]\tvalid_0's l2: 0.327287\tvalid_0's auc: 0.982237\n",
      "[149]\tvalid_0's l2: 0.327304\tvalid_0's auc: 0.982272\n",
      "[150]\tvalid_0's l2: 0.327216\tvalid_0's auc: 0.982258\n",
      "[151]\tvalid_0's l2: 0.327184\tvalid_0's auc: 0.98221\n",
      "[152]\tvalid_0's l2: 0.326939\tvalid_0's auc: 0.98225\n",
      "[153]\tvalid_0's l2: 0.326882\tvalid_0's auc: 0.982213\n",
      "[154]\tvalid_0's l2: 0.326777\tvalid_0's auc: 0.982132\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid_0's l2: 0.327304\tvalid_0's auc: 0.982272\n",
      "Save model...\n",
      "Start predicting...\n",
      "error=0.688000\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 加载你的数据\n",
    "# print('Load data...')\n",
    "# df_train = pd.read_csv('../regression/regression.train', header=None, sep='\\t')\n",
    "# df_test = pd.read_csv('../regression/regression.test', header=None, sep='\\t')\n",
    "#\n",
    "# y_train = df_train[0].values\n",
    "# y_test = df_test[0].values\n",
    "# X_train = df_train.drop(0, axis=1).values\n",
    "# X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "# 创建成lgb特征的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train)  # 将数据保存到LightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  # 创建验证数据\n",
    "\n",
    "# 将参数写成字典下形式\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  # 设置提升类型\n",
    "    'objective': 'regression',  # 目标函数\n",
    "    'metric': {'l2', 'auc'},  # 评估函数\n",
    "    'num_leaves': 31,  # 叶子节点数\n",
    "    'learning_rate': 0.05,  # 学习速率\n",
    "    'feature_fraction': 0.9,  # 建树的特征选择比例\n",
    "    'bagging_fraction': 0.8,  # 建树的样本采样比例\n",
    "    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "    'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)  # 训练数据需要参数列表和数据集\n",
    "\n",
    "print('Save model...')\n",
    "\n",
    "gbm.save_model('model.txt')  # 训练后保存模型到文件\n",
    "\n",
    "print('Start predicting...')\n",
    "# 预测数据集\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration\n",
    "                     )  #如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测\n",
    "# 评估模型\n",
    "print('error=%f' %\n",
    "      (sum(1\n",
    "           for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) /\n",
    "       float(len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca2e65",
   "metadata": {},
   "source": [
    "## 2.scikit-learn接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7140ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    boosting_type='gbdt',  # 提升树的类型 gbdt,dart,goss,rf\n",
    "    num_leaves=31,  #树的最大叶子数，对比xgboost一般为2^(max_depth)\n",
    "    max_depth=-1,  #最大树的深度\n",
    "    learning_rate=0.1,  #学习率\n",
    "    n_estimators=100,  # 拟合的树的棵树，相当于训练轮数\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,  # 最小分割增益\n",
    "    min_child_weight=0.001,  # 分支结点的最小权重\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,  # 训练样本采样率 行\n",
    "    subsample_freq=0,  # 子样本频率\n",
    "    colsample_bytree=1.0,  # 训练特征采样率 列\n",
    "    reg_alpha=0.0,  # L1正则化系数\n",
    "    reg_lambda=0.0,  # L2正则化系数\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "#设置验证集合 verbose=False不打印过程\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f1ef3",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1.https://xgboost.readthedocs.io/\n",
    "\n",
    "2.https://lightgbm.readthedocs.io/\n",
    "\n",
    "3.https://blog.csdn.net/q383700092/article/details/53763328?locationNum=9&fps=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
