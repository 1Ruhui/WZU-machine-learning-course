{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8f4f85",
   "metadata": {},
   "source": [
    "# 机器学习练习8 集成学习\n",
    "\n",
    "代码修改并注释：黄海广，haiguang2000@wzu.edu.cn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3257f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc304344",
   "metadata": {},
   "source": [
    "## 生成数据\n",
    "生成12000行的数据，训练集和测试集按照3:1划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebd8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "data, target = make_hastie_10_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6dfeb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 10), (3000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ac5c7",
   "metadata": {},
   "source": [
    "## 模型对比\n",
    "对比六大模型，都使用默认参数，因为数据是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f808ac16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8dfea3b30f43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m ]):\n\u001b[0;32m     21\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mrunning_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf6 = LGBMClassifier()\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], [\n",
    "        'Logistic Regression', 'Random Forest', 'AdaBoost', 'GBDT', 'XGBoost',\n",
    "        'LightGBM'\n",
    "]):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    end = time.time()\n",
    "    running_time = end - start\n",
    "    print(\"Accuracy: %0.8f (+/- %0.2f),耗时%0.2f秒。模型名称[%s]\" %\n",
    "          (scores.mean(), scores.std(), running_time, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa60220",
   "metadata": {},
   "source": [
    "对比了六大模型，可以看出，逻辑回归速度最快，但准确率最低。\n",
    "而LightGBM，速度快，而且准确率最高，所以，现在处理结构化数据的时候，大部分都是用LightGBM算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e7313",
   "metadata": {},
   "source": [
    "## XGBoost的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5b733",
   "metadata": {},
   "source": [
    "### 1.原生XGBoost的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d20084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#记录程序运行时间\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#xgb矩阵赋值\n",
    "xgb_train = xgb.DMatrix(X_train, y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "##参数\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'silent': 1,  #设置成1则没有运行信息输出，最好是设置为0.\n",
    "    #'nthread':7,# cpu 线程数 默认最大\n",
    "    'eta': 0.007,  # 如同学习率\n",
    "    'min_child_weight': 3,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    'max_depth': 6,  # 构建树的深度，越大越容易过拟合\n",
    "    'gamma': 0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    'subsample': 0.7,  # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,  # 生成树时进行的列采样 \n",
    "    'lambda': 2,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #'alpha':0, # L1 正则项参数\n",
    "    #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。\n",
    "    #'objective': 'multi:softmax', #多分类的问题\n",
    "    #'num_class':10, # 类别数，多分类与 multisoftmax 并用\n",
    "    'seed': 1000,  #随机种子\n",
    "    #'eval_metric': 'auc'\n",
    "}\n",
    "plst = list(params.items())\n",
    "num_rounds = 500  # 迭代次数\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型并保存\n",
    "# early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练\n",
    "model = xgb.train(\n",
    "    plst,\n",
    "    xgb_train,\n",
    "    num_rounds,\n",
    "    watchlist,\n",
    "    early_stopping_rounds=100,\n",
    ")\n",
    "#model.save_model('./model/xgb.model') # 用于存储训练出的模型\n",
    "print(\"best best_ntree_limit\", model.best_ntree_limit)\n",
    "y_pred = model.predict(xgb_test, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "# print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, y_pred))\n",
    "print('error=%f' %\n",
    "      (sum(1\n",
    "           for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) /\n",
    "       float(len(y_pred))))\n",
    "#输出运行时长\n",
    "cost_time = time.time() - start_time\n",
    "print(\"xgboost success!\", '\\n', \"cost time:\", cost_time, \"(s)......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee89f1",
   "metadata": {},
   "source": [
    "### 2.使用scikit-learn接口\n",
    "会改变的函数名是：\n",
    "\n",
    "eta -> learning_rate\n",
    "\n",
    "lambda -> reg_lambda\n",
    "\n",
    "alpha -> reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    silent=0,  #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "    #nthread=4,# cpu 线程数 默认最大\n",
    "    learning_rate=0.3,  # 如同学习率\n",
    "    min_child_weight=1,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    max_depth=6,  # 构建树的深度，越大越容易过拟合\n",
    "    gamma=0,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    subsample=1,  # 随机采样训练样本 训练实例的子采样比\n",
    "    max_delta_step=0,  #最大增量步长，我们允许每个树的权重估计。\n",
    "    colsample_bytree=1,  # 生成树时进行的列采样 \n",
    "    reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #reg_alpha=0, # L1 正则项参数\n",
    "    #scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "    #objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "    #num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "    n_estimators=100,  #树的个数\n",
    "    seed=1000  #随机种子\n",
    "    #eval_metric= 'auc'\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "#设置验证集合 verbose=False不打印过程\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6196fe2",
   "metadata": {},
   "source": [
    "## LIghtGBM的使用\n",
    "### 1.原生接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fa87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.005556\n",
      "[1]\tvalid_0's auc: 0.814872\tvalid_0's l2: 0.965425\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's auc: 0.85319\tvalid_0's l2: 0.933338\n",
      "[3]\tvalid_0's auc: 0.882784\tvalid_0's l2: 0.902614\n",
      "[4]\tvalid_0's auc: 0.885949\tvalid_0's l2: 0.876542\n",
      "[5]\tvalid_0's auc: 0.894002\tvalid_0's l2: 0.850606\n",
      "[6]\tvalid_0's auc: 0.903917\tvalid_0's l2: 0.82653\n",
      "[7]\tvalid_0's auc: 0.906893\tvalid_0's l2: 0.804354\n",
      "[8]\tvalid_0's auc: 0.912181\tvalid_0's l2: 0.782403\n",
      "[9]\tvalid_0's auc: 0.916828\tvalid_0's l2: 0.762403\n",
      "[10]\tvalid_0's auc: 0.9183\tvalid_0's l2: 0.743952\n",
      "[11]\tvalid_0's auc: 0.920517\tvalid_0's l2: 0.727646\n",
      "[12]\tvalid_0's auc: 0.923321\tvalid_0's l2: 0.711146\n",
      "[13]\tvalid_0's auc: 0.923135\tvalid_0's l2: 0.696051\n",
      "[14]\tvalid_0's auc: 0.926152\tvalid_0's l2: 0.68035\n",
      "[15]\tvalid_0's auc: 0.929847\tvalid_0's l2: 0.665401\n",
      "[16]\tvalid_0's auc: 0.930861\tvalid_0's l2: 0.652903\n",
      "[17]\tvalid_0's auc: 0.932855\tvalid_0's l2: 0.638233\n",
      "[18]\tvalid_0's auc: 0.935026\tvalid_0's l2: 0.627191\n",
      "[19]\tvalid_0's auc: 0.936563\tvalid_0's l2: 0.615341\n",
      "[20]\tvalid_0's auc: 0.938868\tvalid_0's l2: 0.604231\n",
      "[21]\tvalid_0's auc: 0.940938\tvalid_0's l2: 0.59298\n",
      "[22]\tvalid_0's auc: 0.942069\tvalid_0's l2: 0.581795\n",
      "[23]\tvalid_0's auc: 0.943108\tvalid_0's l2: 0.573102\n",
      "[24]\tvalid_0's auc: 0.944303\tvalid_0's l2: 0.563376\n",
      "[25]\tvalid_0's auc: 0.945115\tvalid_0's l2: 0.556383\n",
      "[26]\tvalid_0's auc: 0.94597\tvalid_0's l2: 0.548509\n",
      "[27]\tvalid_0's auc: 0.94731\tvalid_0's l2: 0.54173\n",
      "[28]\tvalid_0's auc: 0.948307\tvalid_0's l2: 0.534693\n",
      "[29]\tvalid_0's auc: 0.949068\tvalid_0's l2: 0.527134\n",
      "[30]\tvalid_0's auc: 0.94947\tvalid_0's l2: 0.520536\n",
      "[31]\tvalid_0's auc: 0.9501\tvalid_0's l2: 0.513098\n",
      "[32]\tvalid_0's auc: 0.950378\tvalid_0's l2: 0.506855\n",
      "[33]\tvalid_0's auc: 0.950805\tvalid_0's l2: 0.500827\n",
      "[34]\tvalid_0's auc: 0.951486\tvalid_0's l2: 0.495259\n",
      "[35]\tvalid_0's auc: 0.951876\tvalid_0's l2: 0.489773\n",
      "[36]\tvalid_0's auc: 0.952643\tvalid_0's l2: 0.484072\n",
      "[37]\tvalid_0's auc: 0.952847\tvalid_0's l2: 0.479034\n",
      "[38]\tvalid_0's auc: 0.953325\tvalid_0's l2: 0.47383\n",
      "[39]\tvalid_0's auc: 0.953871\tvalid_0's l2: 0.469013\n",
      "[40]\tvalid_0's auc: 0.95457\tvalid_0's l2: 0.464851\n",
      "[41]\tvalid_0's auc: 0.95495\tvalid_0's l2: 0.459312\n",
      "[42]\tvalid_0's auc: 0.955571\tvalid_0's l2: 0.453869\n",
      "[43]\tvalid_0's auc: 0.956177\tvalid_0's l2: 0.449936\n",
      "[44]\tvalid_0's auc: 0.95682\tvalid_0's l2: 0.445743\n",
      "[45]\tvalid_0's auc: 0.957706\tvalid_0's l2: 0.442063\n",
      "[46]\tvalid_0's auc: 0.958317\tvalid_0's l2: 0.438998\n",
      "[47]\tvalid_0's auc: 0.959156\tvalid_0's l2: 0.435737\n",
      "[48]\tvalid_0's auc: 0.959694\tvalid_0's l2: 0.43265\n",
      "[49]\tvalid_0's auc: 0.960294\tvalid_0's l2: 0.429663\n",
      "[50]\tvalid_0's auc: 0.960809\tvalid_0's l2: 0.426844\n",
      "[51]\tvalid_0's auc: 0.96112\tvalid_0's l2: 0.424183\n",
      "[52]\tvalid_0's auc: 0.961544\tvalid_0's l2: 0.421486\n",
      "[53]\tvalid_0's auc: 0.962023\tvalid_0's l2: 0.418711\n",
      "[54]\tvalid_0's auc: 0.96248\tvalid_0's l2: 0.4164\n",
      "[55]\tvalid_0's auc: 0.962796\tvalid_0's l2: 0.413499\n",
      "[56]\tvalid_0's auc: 0.963001\tvalid_0's l2: 0.410867\n",
      "[57]\tvalid_0's auc: 0.963727\tvalid_0's l2: 0.408386\n",
      "[58]\tvalid_0's auc: 0.964557\tvalid_0's l2: 0.405476\n",
      "[59]\tvalid_0's auc: 0.964944\tvalid_0's l2: 0.402908\n",
      "[60]\tvalid_0's auc: 0.965456\tvalid_0's l2: 0.40051\n",
      "[61]\tvalid_0's auc: 0.965839\tvalid_0's l2: 0.398324\n",
      "[62]\tvalid_0's auc: 0.966073\tvalid_0's l2: 0.396385\n",
      "[63]\tvalid_0's auc: 0.966468\tvalid_0's l2: 0.394467\n",
      "[64]\tvalid_0's auc: 0.966821\tvalid_0's l2: 0.392425\n",
      "[65]\tvalid_0's auc: 0.967089\tvalid_0's l2: 0.390144\n",
      "[66]\tvalid_0's auc: 0.967576\tvalid_0's l2: 0.38792\n",
      "[67]\tvalid_0's auc: 0.968046\tvalid_0's l2: 0.385805\n",
      "[68]\tvalid_0's auc: 0.968426\tvalid_0's l2: 0.383711\n",
      "[69]\tvalid_0's auc: 0.968719\tvalid_0's l2: 0.381824\n",
      "[70]\tvalid_0's auc: 0.969046\tvalid_0's l2: 0.380331\n",
      "[71]\tvalid_0's auc: 0.969303\tvalid_0's l2: 0.378761\n",
      "[72]\tvalid_0's auc: 0.969558\tvalid_0's l2: 0.377396\n",
      "[73]\tvalid_0's auc: 0.970009\tvalid_0's l2: 0.375989\n",
      "[74]\tvalid_0's auc: 0.97039\tvalid_0's l2: 0.374416\n",
      "[75]\tvalid_0's auc: 0.970721\tvalid_0's l2: 0.373099\n",
      "[76]\tvalid_0's auc: 0.97101\tvalid_0's l2: 0.371962\n",
      "[77]\tvalid_0's auc: 0.971243\tvalid_0's l2: 0.370763\n",
      "[78]\tvalid_0's auc: 0.971384\tvalid_0's l2: 0.369558\n",
      "[79]\tvalid_0's auc: 0.971412\tvalid_0's l2: 0.368687\n",
      "[80]\tvalid_0's auc: 0.971794\tvalid_0's l2: 0.367268\n",
      "[81]\tvalid_0's auc: 0.972116\tvalid_0's l2: 0.366005\n",
      "[82]\tvalid_0's auc: 0.972405\tvalid_0's l2: 0.364774\n",
      "[83]\tvalid_0's auc: 0.972764\tvalid_0's l2: 0.363506\n",
      "[84]\tvalid_0's auc: 0.972907\tvalid_0's l2: 0.362201\n",
      "[85]\tvalid_0's auc: 0.973069\tvalid_0's l2: 0.361215\n",
      "[86]\tvalid_0's auc: 0.973469\tvalid_0's l2: 0.359891\n",
      "[87]\tvalid_0's auc: 0.973797\tvalid_0's l2: 0.358739\n",
      "[88]\tvalid_0's auc: 0.973998\tvalid_0's l2: 0.357782\n",
      "[89]\tvalid_0's auc: 0.974226\tvalid_0's l2: 0.356833\n",
      "[90]\tvalid_0's auc: 0.974522\tvalid_0's l2: 0.355635\n",
      "[91]\tvalid_0's auc: 0.974646\tvalid_0's l2: 0.354917\n",
      "[92]\tvalid_0's auc: 0.974738\tvalid_0's l2: 0.354206\n",
      "[93]\tvalid_0's auc: 0.974895\tvalid_0's l2: 0.353465\n",
      "[94]\tvalid_0's auc: 0.975069\tvalid_0's l2: 0.352931\n",
      "[95]\tvalid_0's auc: 0.975241\tvalid_0's l2: 0.352126\n",
      "[96]\tvalid_0's auc: 0.975413\tvalid_0's l2: 0.351453\n",
      "[97]\tvalid_0's auc: 0.975555\tvalid_0's l2: 0.350501\n",
      "[98]\tvalid_0's auc: 0.975693\tvalid_0's l2: 0.349598\n",
      "[99]\tvalid_0's auc: 0.975788\tvalid_0's l2: 0.349073\n",
      "[100]\tvalid_0's auc: 0.975983\tvalid_0's l2: 0.348278\n",
      "[101]\tvalid_0's auc: 0.976195\tvalid_0's l2: 0.347626\n",
      "[102]\tvalid_0's auc: 0.97641\tvalid_0's l2: 0.346756\n",
      "[103]\tvalid_0's auc: 0.976552\tvalid_0's l2: 0.346104\n",
      "[104]\tvalid_0's auc: 0.976678\tvalid_0's l2: 0.345458\n",
      "[105]\tvalid_0's auc: 0.976845\tvalid_0's l2: 0.344811\n",
      "[106]\tvalid_0's auc: 0.976981\tvalid_0's l2: 0.344368\n",
      "[107]\tvalid_0's auc: 0.977029\tvalid_0's l2: 0.344057\n",
      "[108]\tvalid_0's auc: 0.977159\tvalid_0's l2: 0.343704\n",
      "[109]\tvalid_0's auc: 0.977334\tvalid_0's l2: 0.342941\n",
      "[110]\tvalid_0's auc: 0.977355\tvalid_0's l2: 0.34258\n",
      "[111]\tvalid_0's auc: 0.977506\tvalid_0's l2: 0.341977\n",
      "[112]\tvalid_0's auc: 0.977602\tvalid_0's l2: 0.341338\n",
      "[113]\tvalid_0's auc: 0.977798\tvalid_0's l2: 0.340799\n",
      "[114]\tvalid_0's auc: 0.977916\tvalid_0's l2: 0.340182\n",
      "[115]\tvalid_0's auc: 0.977951\tvalid_0's l2: 0.339915\n",
      "[116]\tvalid_0's auc: 0.97798\tvalid_0's l2: 0.339608\n",
      "[117]\tvalid_0's auc: 0.978125\tvalid_0's l2: 0.339209\n",
      "[118]\tvalid_0's auc: 0.978212\tvalid_0's l2: 0.338842\n",
      "[119]\tvalid_0's auc: 0.978338\tvalid_0's l2: 0.338343\n",
      "[120]\tvalid_0's auc: 0.978406\tvalid_0's l2: 0.33814\n",
      "[121]\tvalid_0's auc: 0.978497\tvalid_0's l2: 0.33779\n",
      "[122]\tvalid_0's auc: 0.978643\tvalid_0's l2: 0.337334\n",
      "[123]\tvalid_0's auc: 0.978754\tvalid_0's l2: 0.33696\n",
      "[124]\tvalid_0's auc: 0.978855\tvalid_0's l2: 0.336543\n",
      "[125]\tvalid_0's auc: 0.978962\tvalid_0's l2: 0.33611\n",
      "[126]\tvalid_0's auc: 0.979088\tvalid_0's l2: 0.335631\n",
      "[127]\tvalid_0's auc: 0.97911\tvalid_0's l2: 0.334864\n",
      "[128]\tvalid_0's auc: 0.979262\tvalid_0's l2: 0.334542\n",
      "[129]\tvalid_0's auc: 0.979318\tvalid_0's l2: 0.334262\n",
      "[130]\tvalid_0's auc: 0.979307\tvalid_0's l2: 0.334243\n",
      "[131]\tvalid_0's auc: 0.979368\tvalid_0's l2: 0.334087\n",
      "[132]\tvalid_0's auc: 0.979444\tvalid_0's l2: 0.333897\n",
      "[133]\tvalid_0's auc: 0.979579\tvalid_0's l2: 0.333667\n",
      "[134]\tvalid_0's auc: 0.979682\tvalid_0's l2: 0.333307\n",
      "[135]\tvalid_0's auc: 0.979801\tvalid_0's l2: 0.332936\n",
      "[136]\tvalid_0's auc: 0.979917\tvalid_0's l2: 0.332584\n",
      "[137]\tvalid_0's auc: 0.979967\tvalid_0's l2: 0.332359\n",
      "[138]\tvalid_0's auc: 0.980039\tvalid_0's l2: 0.332196\n",
      "[139]\tvalid_0's auc: 0.980054\tvalid_0's l2: 0.332082\n",
      "[140]\tvalid_0's auc: 0.98009\tvalid_0's l2: 0.331924\n",
      "[141]\tvalid_0's auc: 0.98006\tvalid_0's l2: 0.331995\n",
      "[142]\tvalid_0's auc: 0.980028\tvalid_0's l2: 0.33185\n",
      "[143]\tvalid_0's auc: 0.980009\tvalid_0's l2: 0.331614\n",
      "[144]\tvalid_0's auc: 0.98\tvalid_0's l2: 0.331469\n",
      "[145]\tvalid_0's auc: 0.979971\tvalid_0's l2: 0.331388\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's auc: 0.98009\tvalid_0's l2: 0.331924\n",
      "Save model...\n",
      "Start predicting...\n",
      "error=0.677667\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 加载你的数据\n",
    "# print('Load data...')\n",
    "# df_train = pd.read_csv('../regression/regression.train', header=None, sep='\\t')\n",
    "# df_test = pd.read_csv('../regression/regression.test', header=None, sep='\\t')\n",
    "#\n",
    "# y_train = df_train[0].values\n",
    "# y_test = df_test[0].values\n",
    "# X_train = df_train.drop(0, axis=1).values\n",
    "# X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "# 创建成lgb特征的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train)  # 将数据保存到LightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  # 创建验证数据\n",
    "\n",
    "# 将参数写成字典下形式\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  # 设置提升类型\n",
    "    'objective': 'regression',  # 目标函数\n",
    "    'metric': {'l2', 'auc'},  # 评估函数\n",
    "    'num_leaves': 31,  # 叶子节点数\n",
    "    'learning_rate': 0.05,  # 学习速率\n",
    "    'feature_fraction': 0.9,  # 建树的特征选择比例\n",
    "    'bagging_fraction': 0.8,  # 建树的样本采样比例\n",
    "    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "    'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)  # 训练数据需要参数列表和数据集\n",
    "\n",
    "print('Save model...')\n",
    "\n",
    "gbm.save_model('model.txt')  # 训练后保存模型到文件\n",
    "\n",
    "print('Start predicting...')\n",
    "# 预测数据集\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration\n",
    "                     )  #如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测\n",
    "# 评估模型\n",
    "print('error=%f' %\n",
    "      (sum(1\n",
    "           for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) /\n",
    "       float(len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca2e65",
   "metadata": {},
   "source": [
    "## 2.scikit-learn接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7140ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9303\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    boosting_type='gbdt',  # 提升树的类型 gbdt,dart,goss,rf\n",
    "    num_leaves=31,  #树的最大叶子数，对比xgboost一般为2^(max_depth)\n",
    "    max_depth=-1,  #最大树的深度\n",
    "    learning_rate=0.1,  #学习率\n",
    "    n_estimators=100,  # 拟合的树的棵树，相当于训练轮数\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,  # 最小分割增益\n",
    "    min_child_weight=0.001,  # 分支结点的最小权重\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,  # 训练样本采样率 行\n",
    "    subsample_freq=0,  # 子样本频率\n",
    "    colsample_bytree=1.0,  # 训练特征采样率 列\n",
    "    reg_alpha=0.0,  # L1正则化系数\n",
    "    reg_lambda=0.0,  # L2正则化系数\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "#设置验证集合 verbose=False不打印过程\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f1ef3",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1.https://xgboost.readthedocs.io/\n",
    "\n",
    "2.https://lightgbm.readthedocs.io/\n",
    "\n",
    "3.https://blog.csdn.net/q383700092/article/details/53763328?locationNum=9&fps=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
